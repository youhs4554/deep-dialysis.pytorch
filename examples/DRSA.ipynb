{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1351585f",
   "metadata": {},
   "source": [
    "## DRSA : Deep Recurrent Survival Analysis\n",
    "- paper : https://arxiv.org/pdf/1809.02403.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18321bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd30b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drsa.functions import event_time_loss, event_rate_loss\n",
    "from drsa.model import DRSA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pycox.datasets as dt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb420631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "from torch.utils.data import Dataset\n",
    "from pycox.preprocessing import label_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19e8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRSA_Dataset(Dataset):\n",
    "    def __init__(self, name, phase='train'):\n",
    "        super().__init__()\n",
    "        if name == 'support':\n",
    "            cols_standardize =  ['x0', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13']\n",
    "            cols_leave = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
    "            max_seq_len = 25 # will be modified\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.cols_standardize = cols_standardize\n",
    "        self.cols_leave = cols_leave\n",
    "        \n",
    "        cols_tgt = ['duration', 'event']\n",
    "            \n",
    "        df_full = eval(f'dt.{name}.read_df()')\n",
    "        df_test = df_full.sample(frac=0.3)\n",
    "        df_train = df_full.drop(df_test.index)\n",
    "        df_val = df_train.sample(frac=0.1)\n",
    "        df_train = df_train.drop(df_val.index)\n",
    "        \n",
    "        # Target info\n",
    "        y_train = df_train[cols_tgt].values\n",
    "        y_val = df_val[cols_tgt].values\n",
    "        y_test = df_test[cols_tgt].values\n",
    "        \n",
    "        # Target label preprocessing\n",
    "        target_parse_fn = DRSA_Dataset.get_target\n",
    "        labtrans = label_transforms.LabTransDiscreteTime(max_seq_len)\n",
    "        y_train  = np.c_[labtrans.fit_transform(*target_parse_fn(y_train))]\n",
    "        y_val    = np.c_[labtrans.transform(*target_parse_fn(y_val))]\n",
    "        y_test    = np.c_[labtrans.transform(*target_parse_fn(y_test))]\n",
    "        \n",
    "        # Input Covariates\n",
    "        df_train = df_train.drop(cols_tgt, axis=1)\n",
    "        df_val = df_val.drop(cols_tgt, axis=1)\n",
    "        df_test = df_test.drop(cols_tgt, axis=1)\n",
    "        \n",
    "        # Input data preprocessing\n",
    "        standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "        leave = [(col, None) for col in cols_leave]\n",
    "        \n",
    "        # assume categorical columns located first (for embedding later)\n",
    "        x_mapper = DataFrameMapper(leave + standardize)\n",
    "        x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "        x_val = x_mapper.transform(df_val).astype('float32')\n",
    "        x_test = x_mapper.transform(df_test).astype('float32')\n",
    "        \n",
    "        if phase == 'train':\n",
    "            self.X, self.Y = x_train, y_train\n",
    "        if phase == 'val':\n",
    "            self.X, self.Y = x_val, y_val\n",
    "        if phase == 'test':\n",
    "            self.X, self.Y = x_test, y_test\n",
    "        \n",
    "        setattr(self, 'labtrans', labtrans)\n",
    "                    \n",
    "    @property\n",
    "    def numeric_columns(self):\n",
    "        return self.cols_standardize\n",
    "    \n",
    "    @property\n",
    "    def categorical_columns(self):\n",
    "        return self.cols_leave\n",
    "    \n",
    "    @property\n",
    "    def n_features(self):\n",
    "        return self.X.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def n_embeddings(self):\n",
    "        return [ len(np.unique(self.X[:, ix])) for ix in range(len(self.categorical_columns)) ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_target(y):\n",
    "        durations, events = y[:, 0], y[:, 1]\n",
    "        return durations, events\n",
    "        \n",
    "    def __getitem__(self, ix):\n",
    "        x = torch.from_numpy(self.X[ix])\n",
    "        y = torch.from_numpy(self.Y[ix])\n",
    "        \n",
    "        # Add time features on tiled covariates\n",
    "        xs = x.tile(self.max_seq_len, 1)\n",
    "        t = torch.arange(self.max_seq_len).unsqueeze(1)\n",
    "        xs = torch.cat((xs, t), dim=1)\n",
    "        \n",
    "        return xs, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97e24d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "name = 'support'\n",
    "\n",
    "train_ds = DRSA_Dataset(name, 'train')\n",
    "val_ds = DRSA_Dataset(name, 'val')\n",
    "test_ds = DRSA_Dataset(name, 'test')\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec295bac",
   "metadata": {},
   "source": [
    "## instantiating embedding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c11b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ea7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 5\n",
    "\n",
    "get_embeddings = lambda n_embeddings: [torch.nn.Embedding(nb_emb, embedding_size, device=device) for nb_emb in n_embeddings ]\n",
    "embeddings = get_embeddings(train_ds.n_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f8ba0",
   "metadata": {},
   "source": [
    "## instantiating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5064aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchtuples as tt\n",
    "from pycox import models\n",
    "from pycox.models.utils import pad_col\n",
    "from pycox.models.interpolation import InterpolatePMF\n",
    "from pycox.evaluation.concordance import concordance_td\n",
    "from pycox import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02be41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyCoxWrapper(tt.Model):\n",
    "    \"\"\"Wrapper class for pycox API compatibility\n",
    "    \"\"\"\n",
    "    _steps ='post'\n",
    "    def __init__(self, net, loss=None, optimizer=None, device=None, duration_index=None):\n",
    "        self.duration_index = duration_index\n",
    "        super().__init__(net, loss, optimizer, device)\n",
    "\n",
    "    @property\n",
    "    def duration_index(self):\n",
    "        \"\"\"\n",
    "        Array of durations that defines the discrete times. This is used to set the index\n",
    "        of the DataFrame in `predict_surv_df`.\n",
    "        \n",
    "        Returns:\n",
    "            np.array -- Duration index.\n",
    "        \"\"\"\n",
    "        return self._duration_index\n",
    "\n",
    "    @duration_index.setter\n",
    "    def duration_index(self, val):\n",
    "        self._duration_index = val\n",
    "        \n",
    "    @property\n",
    "    def index_surv(self):\n",
    "        return self.surv.index.values\n",
    "\n",
    "    @property\n",
    "    def steps(self):\n",
    "        \"\"\"How to handle predictions that are between two indexes in `index_surv`.\n",
    "\n",
    "        For a visualization, run the following:\n",
    "            ev = EvalSurv(pd.DataFrame(np.linspace(1, 0, 7)), np.empty(7), np.ones(7), steps='pre')\n",
    "            ax = ev[0].plot_surv()\n",
    "            ev.steps = 'post'\n",
    "            ev[0].plot_surv(ax=ax, style='--')\n",
    "            ax.legend(['pre', 'post'])\n",
    "        \"\"\"\n",
    "        return self._steps\n",
    "\n",
    "    @steps.setter\n",
    "    def steps(self, steps):\n",
    "        vals = ['post', 'pre']\n",
    "        if steps not in vals:\n",
    "            raise ValueError(f\"`steps` needs to be {vals}, got {steps}\")\n",
    "        self._steps = steps\n",
    "\n",
    "\n",
    "    def idx_at_times(self, times):\n",
    "        \"\"\"Get the index (iloc) of the `surv.index` closest to `times`.\n",
    "        I.e. surv.loc[tims] (almost)= surv.iloc[idx_at_times(times)].\n",
    "\n",
    "        Useful for finding predictions at given durations.\n",
    "        \"\"\"\n",
    "        return utils.idx_at_times(self.index_surv, times, self.steps)\n",
    "    \n",
    "    def predict_surv(self, input, batch_size=8224, numpy=None, eval_=True, to_cpu=False,\n",
    "                     num_workers=0):\n",
    "        pmf = self.predict_pmf(input, batch_size, False, eval_, to_cpu, num_workers)\n",
    "        surv = 1 - pmf.cumsum(1)\n",
    "        return tt.utils.array_or_tensor(surv, numpy, input)\n",
    "\n",
    "    def predict_pmf(self, input, batch_size=8224, numpy=None, eval_=True, to_cpu=False,\n",
    "                    num_workers=0):\n",
    "        preds = self.predict(input, batch_size, False, eval_, False, to_cpu, num_workers)\n",
    "        pmf = pad_col(preds).softmax(1)[:, :-1]\n",
    "        return tt.utils.array_or_tensor(pmf, numpy, input)\n",
    "\n",
    "    def predict_surv_df(self, input, batch_size=8224, eval_=True, num_workers=0):\n",
    "        surv = self.predict_surv(input, batch_size, True, eval_, True, num_workers)\n",
    "        return pd.DataFrame(surv.transpose(), self.duration_index)\n",
    "\n",
    "    def interpolate(self, sub=10, scheme='const_pdf', duration_index=None):\n",
    "        \"\"\"Use interpolation for predictions.\n",
    "        There are only one scheme:\n",
    "            `const_pdf` and `lin_surv` which assumes pice-wise constant pmf in each interval (linear survival).\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            sub {int} -- Number of \"sub\" units in interpolation grid. If `sub` is 10 we have a grid with\n",
    "                10 times the number of grid points than the original `duration_index` (default: {10}).\n",
    "            scheme {str} -- Type of interpolation {'const_hazard', 'const_pdf'}.\n",
    "                See `InterpolateDiscrete` (default: {'const_pdf'})\n",
    "            duration_index {np.array} -- Cuts used for discretization. Does not affect interpolation,\n",
    "                only for setting index in `predict_surv_df` (default: {None})\n",
    "        \n",
    "        Returns:\n",
    "            [InterpolationPMF] -- Object for prediction with interpolation.\n",
    "        \"\"\"\n",
    "        if duration_index is None:\n",
    "            duration_index = self.duration_index\n",
    "        return InterpolatePMF(self, scheme, duration_index, sub)\n",
    "    \n",
    "    def concordance_td(self, preds, target, method='adj_antolini'):\n",
    "        \"\"\"Time dependent concorance index from\n",
    "        Antolini, L.; Boracchi, P.; and Biganzoli, E. 2005. A time-dependent discrimination\n",
    "        index for survival data. Statistics in Medicine 24:3927–3944.\n",
    "\n",
    "        If 'method' is 'antolini', the concordance from Antolini et al. is computed.\n",
    "    \n",
    "        If 'method' is 'adj_antolini' (default) we have made a small modifications\n",
    "        for ties in predictions and event times.\n",
    "        We have followed step 3. in Sec 5.1. in Random Survival Forests paper, except for the last\n",
    "        point with \"T_i = T_j, but not both are deaths\", as that doesn't make much sense.\n",
    "        See 'metrics._is_concordant'.\n",
    "\n",
    "        Keyword Arguments:\n",
    "            method {str} -- Type of c-index 'antolini' or 'adj_antolini' (default {'adj_antolini'}).\n",
    "\n",
    "        Returns:\n",
    "            float -- Time dependent concordance index.\n",
    "        \"\"\"\n",
    "        pmf = pad_col(preds.squeeze(2)).softmax(1)[:, :-1]\n",
    "        pmf = tt.utils.array_or_tensor(pmf, True, pmf)\n",
    "        \n",
    "        surv = 1 - pmf.cumsum(1)\n",
    "        self.surv = pd.DataFrame(surv.transpose(), self.duration_index)\n",
    "        \n",
    "        durations, events = np.split(target.cpu().numpy(), 2, axis=1)\n",
    "        durations = durations.ravel()\n",
    "        events = events.ravel()\n",
    "        \n",
    "        duration_idx = self.idx_at_times(durations) # need to debug\n",
    "        \n",
    "        from IPython.core.debugger import set_trace\n",
    "        set_trace()\n",
    "\n",
    "        return concordance_td(durations, events, self.surv.values,\n",
    "                              duration_idx, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8d5f33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DRSA(\n",
       "  (lstm): LSTM(39, 2, batch_first=True)\n",
       "  (fc): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (linear_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (params_to_train): ModuleList(\n",
       "    (0): Embedding(2, 5)\n",
       "    (1): Embedding(10, 5)\n",
       "    (2): Embedding(6, 5)\n",
       "    (3): Embedding(2, 5)\n",
       "    (4): Embedding(2, 5)\n",
       "    (5): Embedding(3, 5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DRSA(\n",
    "    n_features=train_ds.n_features + 1,  # +1 for time features\n",
    "    hidden_dim=2,\n",
    "    n_layers=1,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6ec63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossDRSA(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super().__init__()\n",
    "        assert (alpha >= 0) and (alpha <= 1), 'Need `alpha` in [0, 1].'\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, preds, target=None):\n",
    "        # weighted average of event_time_loss and event_rate_loss\n",
    "        evt_loss = event_time_loss(preds)\n",
    "        evr_loss = event_rate_loss(preds)\n",
    "        loss = (self.alpha * evt_loss) + ((1 - self.alpha) * evr_loss)\n",
    "        return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec3edc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "labtrans = train_ds.labtrans\n",
    "\n",
    "model = PyCoxWrapper(net, LossDRSA(), tt.optim.Adam(lr=1e-3), \n",
    "                     duration_index=labtrans.cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b58f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\youhs\\appdata\\local\\temp\\ipykernel_17804\\3556093577.py\u001b[0m(127)\u001b[0;36mconcordance_td\u001b[1;34m()\u001b[0m\n",
      "\n",
      "ipdb> durations\n",
      "array([ 3.,  7., 13.,  1.,  1.,  6., 20.,  2.,  9.,  1.,  6., 10., 15.,\n",
      "        1.,  9.,  3.,  5.,  3.,  2.,  5.,  1.,  1., 17., 12.,  3.,  1.,\n",
      "        7.,  3.,  5., 19.,  4.,  1.,  1.,  1.,  1.,  3.,  9.,  9., 12.,\n",
      "       17., 20.,  7., 10.,  6., 17.,  5.,  7., 22.,  1., 23.,  1.,  6.,\n",
      "        2., 11.,  7.,  2.,  3., 15.,  2.,  4.,  4., 16.,  6.,  1.,  4.,\n",
      "        1.,  4., 18.,  1., 18.,  5.,  5.,  9., 21.,  6.,  6.,  2.,  1.,\n",
      "        1., 20., 15.,  1., 18.,  1., 11.,  1.,  5.,  1.,  1.,  5.,  1.,\n",
      "        1., 12.,  2.,  1.,  1.,  6.,  2.,  4.,  2.,  1.,  1.,  1., 20.,\n",
      "        1.,  6., 18.,  1.,  5.,  6.,  8.,  1.,  1.,  9.,  2.,  9.,  7.,\n",
      "       23.,  1.,  1.,  5.,  5.,  1.,  2., 22.,  1.,  1.,  8.])\n",
      "ipdb> events\n",
      "array([1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
      "ipdb> self.surv.values\n",
      "array([[0.96115637, 0.96228576, 0.96307117, ..., 0.96357924, 0.9644209 ,\n",
      "        0.96314347],\n",
      "       [0.92207325, 0.92366594, 0.9260491 , ..., 0.9272928 , 0.92859226,\n",
      "        0.9269784 ],\n",
      "       [0.88294923, 0.8844402 , 0.88834536, ..., 0.8902652 , 0.89158046,\n",
      "        0.89111525],\n",
      "       ...,\n",
      "       [0.09871948, 0.09838486, 0.09959626, ..., 0.09986448, 0.09995997,\n",
      "        0.10120451],\n",
      "       [0.05948842, 0.05928636, 0.06001675, ..., 0.06017834, 0.06023586,\n",
      "        0.06098586],\n",
      "       [0.02025729, 0.02018821, 0.02043718, ..., 0.02049214, 0.02051175,\n",
      "        0.02076715]], dtype=float32)\n",
      "ipdb> duration_idx\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)\n",
      "ipdb> concordance_td(durations, events, self.surv.values,                               duration_idx, method)\n",
      "0.5402597402597402\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "log = model.fit_dataloader(train_loader, \n",
    "                           epochs=50, \n",
    "                           val_dataloader=test_loader, \n",
    "                           metrics={'Ctd': model.concordance_td}\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deda10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a191fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
